import os
import json
import logging
import traceback
from pathlib import Path
from typing import Optional, AsyncGenerator, List
from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Request, Body
from fastapi.middleware.cors import CORSMiddleware
import httpx
from dotenv import load_dotenv
from docx import Document
import openpyxl
import pandas as pd
import win32com.client
import pythoncom
import asyncio
import sys
from fastapi.responses import StreamingResponse
import tempfile
import docx
import aiofiles

# 配置详细的日志格式
logging.basicConfig(
    level=logging.DEBUG,  # 改为 DEBUG 级别
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# 确保所有 httpx 的日志也被捕获
logging.getLogger("httpx").setLevel(logging.DEBUG)

# 获取当前文件的目录
current_dir = Path(__file__).resolve().parent
project_root = current_dir.parent.parent

# 尝试加载不同位置的 .env 文件
env_files = [
    project_root / '.env',
    current_dir.parent / '.env',
    current_dir / '.env',
]

env_loaded = False
for env_file in env_files:
    if env_file.exists():
        logger.info(f"找到 .env 文件：{env_file}")
        load_dotenv(env_file)
        env_loaded = True
        break

if not env_loaded:
    logger.error("未找到 .env 文件，请确保文件存在于以下位置之一：")
    for env_file in env_files:
        logger.error(f"- {env_file}")

# 验证环境变量
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
if not DEEPSEEK_API_KEY:
    logger.error("DEEPSEEK_API_KEY 未在环境变量中设置")
    logger.error("当前所有环境变量：")
    for key, value in os.environ.items():
        if 'KEY' in key:  # 只显示包含 'KEY' 的环境变量，并隐藏具体值
            logger.error(f"- {key}: {'*' * len(value)}")
else:
    logger.info("DEEPSEEK_API_KEY 已成功加载")

# API 配置
API_BASE_URL = os.getenv('API_BASE_URL', 'http://localhost:8000')
logger.info(f"Using API base URL: {API_BASE_URL}")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

UPLOAD_DIR = "uploads"
if not os.path.exists(UPLOAD_DIR):
    os.makedirs(UPLOAD_DIR)

# 添加一个常量定义最大处理长度
MAX_CHUNK_SIZE = 4000  # 根据实际模型限制调整

def convert_doc_to_docx(doc_path):
    try:
        pythoncom.CoInitialize()
        try:
            word = win32com.client.Dispatch("Word.Application")
            word.Visible = False
            word.DisplayAlerts = False  # 禁用警告
        except Exception as e:
            raise Exception("无法启动 Word，请确保已安装 Microsoft Word 且以管理员权限运行程序。错误信息：" + str(e))
        
        try:
            # 创建一个新的空白文档
            new_doc = word.Documents.Add()
            
            # 读取原文档内容
            with open(doc_path, 'rb') as file:
                content = file.read()
            
            # 创建临时文件
            temp_path = os.path.join(os.path.dirname(doc_path), "temp_" + os.path.basename(doc_path))
            with open(temp_path, 'wb') as file:
                file.write(content)
            
            try:
                # 尝试打开临时文件
                doc = word.Documents.Open(os.path.abspath(temp_path))
                docx_path = doc_path + "x"  # Add 'x' to make it .docx
                doc.SaveAs2(os.path.abspath(docx_path), FileFormat=16)  # 16 = docx format
                doc.Close()
                return docx_path
            finally:
                # 清理临时文件
                if os.path.exists(temp_path):
                    try:
                        os.remove(temp_path)
                    except:
                        pass
        except Exception as e:
            if "检测到此文件存在一个问题" in str(e):
                raise Exception("Word 安全设置阻止打开文件。请手动打开文件，点击'启用编辑'，然后重新上传，或者将文件另存为 .docx 格式后重试。")
            raise Exception("转换文档时出错，请确保文档未被其他程序占用。错误信息：" + str(e))
        finally:
            try:
                word.Quit()
            except:
                pass
    except Exception as e:
        raise Exception(f"文档转换失败: {str(e)}")
    finally:
        pythoncom.CoUninitialize()

def read_document(file_path):
    try:
        logger.info(f"开始读取文档: {file_path}")
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if not os.path.exists(file_path):
            raise Exception(f"文件不存在: {file_path}")
            
        logger.info(f"文件格式: {file_ext}")
        
        try:
            if file_ext == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    text = f.read()
                logger.info("成功读取文本文件")
            elif file_ext == '.doc':
                # 将 .doc 转换为 .docx
                docx_path = convert_doc_to_docx(file_path)
                doc = Document(docx_path)
                
                # 读取段落文本
                paragraphs_text = [paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]
                
                # 读取表格内容
                tables_text = []
                for table in doc.tables:
                    for row in table.rows:
                        row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]
                        if row_text:  # 只添加非空行
                            tables_text.append(" | ".join(row_text))
                
                # 合并段落和表格内容
                text = "\n\n".join(paragraphs_text)
                if tables_text:
                    text += "\n\n表格内容：\n" + "\n".join(tables_text)
                
                # 清理临时文件
                if os.path.exists(docx_path):
                    os.remove(docx_path)
                logger.info("成功读取 DOC 文件")
            elif file_ext == '.docx':
                doc = Document(file_path)
                
                # 读取段落文本
                paragraphs_text = [paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]
                
                # 读取表格内容
                tables_text = []
                for table in doc.tables:
                    for row in table.rows:
                        row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]
                        if row_text:  # 只添加非空行
                            tables_text.append(" | ".join(row_text))
                
                # 合并段落和表格内容
                text = "\n\n".join(paragraphs_text)
                if tables_text:
                    text += "\n\n表格内容：\n" + "\n".join(tables_text)
                    
                logger.info("成功读取 DOCX 文件")
            else:
                raise ValueError(f"不支持的文件格式: {file_ext}")
            
            logger.info(f"文档内容长度: {len(text)} 字符")
            if tables_text:
                logger.info(f"包含表格行数: {len(tables_text)}")
            return text
        except Exception as e:
            logger.error(f"读取文档失败: {str(e)}")
            logger.error(traceback.format_exc())
            raise Exception(f"读取文档失败: {str(e)}")
            
    except Exception as e:
        logger.error(f"处理文档时出错: {str(e)}")
        logger.error(traceback.format_exc())
        raise Exception(f"处理文档时出错: {str(e)}")

def read_school_info(file_path):
    try:
        logger.info(f"开始读取学校信息: {file_path}")
        file_ext = os.path.splitext(file_path)[1].lower()
        
        if not os.path.exists(file_path):
            raise Exception(f"文件不存在: {file_path}")
            
        logger.info(f"文件格式: {file_ext}")
        
        try:
            if file_ext == '.txt':
                # 处理文本文件
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                data = [line.strip().split(',') for line in content.splitlines() if line.strip()]
                formatted_data = {}
                for line in data:
                    if len(line) >= 2:
                        formatted_data[line[0].strip()] = line[1].strip()
            elif file_ext in ['.csv', '.xls', '.xlsx']:
                # 读取 Excel/CSV 文件
                if file_ext == '.csv':
                    df = pd.read_csv(file_path, encoding='utf-8')
                else:
                    df = pd.read_excel(file_path)
                
                logger.info(f"原始数据形状: {df.shape}")
                logger.info(f"列名: {df.columns.tolist()}")
                
                # 初始化结果字典
                formatted_data = {}
                
                # 遍历所有单元格查找关键信息
                for row in range(df.shape[0]):
                    for col in range(df.shape[1]):
                        cell_value = str(df.iloc[row, col]).strip() if pd.notna(df.iloc[row, col]) else ""
                        
                        # 检查是否是关键字段
                        if cell_value in ["在读专业", "本科专业", "当前专业", "申请专业", "目标专业", "意向专业"]:
                            # 获取下一行的值（如果存在）
                            if row + 1 < df.shape[0]:
                                next_value = str(df.iloc[row + 1, col]).strip() if pd.notna(df.iloc[row + 1, col]) else ""
                                if next_value:  # 只保存非空值
                                    if cell_value in ["在读专业", "本科专业", "当前专业"]:
                                        formatted_data["在读专业"] = next_value
                                    elif cell_value in ["申请专业", "目标专业", "意向专业"]:
                                        formatted_data["申请专业"] = next_value
                
                # 如果没有找到任何信息，使用默认值
                if not formatted_data:
                    formatted_data = {
                        "在读专业": "A专业",
                        "申请专业": "B专业"
                    }
                
                logger.info(f"处理后的数据条数: {len(formatted_data)}")
            else:
                raise ValueError(f"不支持的文件格式: {file_ext}")
            
            logger.info(f"最终数据示例: {formatted_data}")
            return formatted_data
            
        except Exception as e:
            logger.error(f"读取表格失败: {str(e)}")
            logger.error(traceback.format_exc())
            raise Exception(f"读取表格失败: {str(e)}")
            
    except Exception as e:
        logger.error(f"处理表格时出错: {str(e)}")
        logger.error(traceback.format_exc())
        raise Exception(f"处理表格时出错: {str(e)}")

async def call_ai_with_retry(prompt: str, model: str = "deepseek-api", temperature: float = 1.5, max_retries: int = 3) -> str:
    """调用AI服务生成文本，支持重试机制"""
    logger.debug("开始调用 AI 服务")
    logger.info(f"模型：{model}，temperature：{temperature}")
    logger.debug(f"提示词长度：{len(prompt)} 字符")
    
    retry_count = 0
    last_error = None
    timeout = httpx.Timeout(60.0, read=120.0)  # 初始超时设置
    
    while retry_count < max_retries:
        try:
            model_lower = model.lower().strip()
            logger.debug(f"尝试第 {retry_count + 1} 次调用，模型：{model_lower}")
            logger.debug(f"当前超时设置：连接 {timeout.connect} 秒，读取 {timeout.read} 秒")
            
            if any(m in model_lower for m in ["deepseek", "deepseek-api", "deepseek-chat"]):
                api_key = os.getenv("DEEPSEEK_API_KEY")
                if not api_key:
                    raise HTTPException(
                        status_code=503,
                        detail="DeepSeek API 密钥未设置，请检查环境变量"
                    )
                
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json"
                }
                
                request_data = {
                    "model": "deepseek-chat",
                    "messages": [
                        {"role": "system", "content": "You are a helpful assistant specialized in writing personal statements."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": temperature,
                    "stream": False,
                    "max_tokens": 4000  # 增加 token 数量
                }
                
                async with httpx.AsyncClient(timeout=timeout) as client:
                    try:
                        response = await client.post(
                            "https://api.deepseek.com/v1/chat/completions",
                            headers=headers,
                            json=request_data
                        )
                        
                        logger.debug(f"收到响应，状态码：{response.status_code}")
                        response.raise_for_status()
                        
                        # 记录原始响应
                        raw_response = response.text
                        logger.debug(f"原始响应：{raw_response[:1000]}")
                        
                        if not raw_response:
                            raise ValueError("API 返回了空响应")
                        
                        # 尝试读取响应内容
                        try:
                            result = response.json()
                            logger.debug(f"API 响应：{result}")
                        except json.JSONDecodeError as e:
                            logger.error(f"JSON 解析错误：{str(e)}")
                            logger.error(f"原始响应：{raw_response}")
                            raise ValueError(f"无效的 JSON 响应：{raw_response[:200]}")
                        
                        if not result or "choices" not in result or not result["choices"]:
                            raise ValueError(f"API 返回了无效格式：{result}")
                        
                        content = result["choices"][0]["message"]["content"]
                        if not content:
                            raise ValueError("API 返回了空内容")
                        
                        logger.info("成功获取 AI 响应")
                        return content
                        
                    except httpx.TimeoutException as e:
                        logger.error(f"请求超时：{str(e)}")
                        raise ValueError(f"API 请求超时：{str(e)}")
                    except httpx.HTTPStatusError as e:
                        logger.error(f"HTTP 错误：{e.response.status_code} - {e.response.text}")
                        raise ValueError(f"API 请求失败：{e.response.text}")
                    except Exception as e:
                        logger.error(f"请求过程中出错：{str(e)}")
                        raise ValueError(f"API 请求过程中出错：{str(e)}")
            else:
                raise ValueError(f"不支持的模型：{model}")
                
        except Exception as e:
            last_error = e
            retry_count += 1
            if retry_count < max_retries:
                wait_time = min(2 ** retry_count, 30)  # 最长等待30秒
                logger.warning(f"调用失败 ({retry_count}/{max_retries}): {str(e)}")
                logger.warning(f"等待 {wait_time} 秒后重试...")
                await asyncio.sleep(wait_time)
                
                # 增加超时时间
                timeout = httpx.Timeout(
                    60.0 * (retry_count + 1),  # 每次重试增加60秒
                    read=120.0 * (retry_count + 1)  # 每次重试增加120秒
                )
            else:
                break
    
    error_msg = f"在 {max_retries} 次尝试后调用仍然失败"
    if last_error:
        error_msg += f"：{str(last_error)}"
    logger.error(error_msg)
    raise HTTPException(status_code=503, detail=error_msg)

@app.post("/upload")
async def upload_files(
    resume: UploadFile = File(...),
    personal_statement: UploadFile = File(...),
    school_info: UploadFile = File(...),
    model: str = Form("deepseek-api"),
    temperature: float = Form(0.7),
    prompt_template: str = Form(None)
):
    try:
        logger.info(f"Received upload request - Files: {resume.filename}, {personal_statement.filename}, {school_info.filename}")
        logger.info(f"Model: {model}, Temperature: {temperature}")
        
        # 初始化上传文件列表用于跟踪清理
        uploaded_files = []
        
        ext = os.path.splitext(school_info.filename)[1].lower()
        if ext not in ['.xls', '.xlsx', '.csv', '.txt']:
            raise Exception(f"文件 {school_info.filename} 格式不正确。请上传 .xls、.xlsx、.csv 或 .txt 格式的文件。")

        # 创建上传目录
        if not os.path.exists(UPLOAD_DIR):
            try:
                os.makedirs(UPLOAD_DIR)
                logger.info(f"创建上传目录: {UPLOAD_DIR}")
            except Exception as e:
                logger.error(f"创建上传目录失败: {str(e)}")
                raise Exception(f"创建上传目录失败: {str(e)}")

        resume_path = os.path.join(UPLOAD_DIR, resume.filename)
        ps_path = os.path.join(UPLOAD_DIR, personal_statement.filename)
        school_path = os.path.join(UPLOAD_DIR, school_info.filename)
        
        logger.info("开始保存上传的文件")
        # Save uploaded files
        try:
            with open(resume_path, "wb") as f:
                content = await resume.read()
                logger.info(f"读取简历文件，大小: {len(content)} 字节")
                f.write(content)
            uploaded_files.append(resume_path)
            
            with open(ps_path, "wb") as f:
                content = await personal_statement.read()
                logger.info(f"读取个人陈述文件，大小: {len(content)} 字节")
                f.write(content)
            uploaded_files.append(ps_path)
            
            with open(school_path, "wb") as f:
                content = await school_info.read()
                logger.info(f"读取学校信息文件，大小: {len(content)} 字节")
                f.write(content)
            uploaded_files.append(school_path)
            
            logger.info("所有文件保存成功")
            
        except Exception as e:
            logger.error(f"保存上传文件失败: {str(e)}")
            logger.error(traceback.format_exc())
            raise Exception(f"保存上传文件失败: {str(e)}")
        
        try:
            # Read documents
            logger.info("开始读取文档内容")
            resume_text = read_document(resume_path)
            logger.info("简历读取完成")
            
            ps_text = read_document(ps_path)
            logger.info("个人陈述读取完成")
            
            # 读取学校信息
            school_info_data = read_school_info(school_path)
            logger.info("学校信息读取完成")
            
            # 使用统一的处理函数
            generated_text = await process_materials(
                resume_text,
                ps_text,
                school_info_data,
                model,
                temperature,
                prompt_template
            )
            
            return {"status": "success", "personal_statement": generated_text}
            
        except Exception as e:
            logger.error(f"处理文件内容时出错: {str(e)}")
            logger.error(traceback.format_exc())
            raise Exception(f"处理文件内容时出错: {str(e)}")
            
    except Exception as e:
        logger.error(f"上传处理失败: {str(e)}")
        logger.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}
    finally:
        # Clean up uploaded files
        logger.info("清理上传的文件")
        for file_path in uploaded_files:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    logger.info(f"删除文件: {file_path}")
            except Exception as e:
                logger.error(f"删除文件失败: {file_path}, 错误: {str(e)}")
                pass  # Ignore cleanup errors

@app.get("/")
async def read_root():
    return {"message": "Personal Statement Generator API"}

@app.post("/continue")
async def continue_generation(
    request: Request,
    model: str = "deepseek-api",
    temperature: float = 1.5
):
    try:
        body = await request.json()
        selected_text = body.get("selected_text")
        prompt = body.get("prompt")
        model = body.get("model", model)
        temperature = body.get("temperature", temperature)

        if not selected_text or not prompt:
            raise HTTPException(status_code=400, detail="Missing selected_text or prompt")

        logger.info(f"继续生成，选中文本: {selected_text[:100]}...")
        logger.info(f"继续生成提示词: {prompt}")
        logger.info(f"使用模型: {model}")
        logger.info(f"Temperature: {temperature}")

        continue_prompt = f"""基于以下文本继续生成或修改：

原文：
{selected_text}

要求：
{prompt}

请保持文风一致，确保生成的内容与上下文自然衔接。
"""

        generated_text = await call_ai_with_retry(continue_prompt, model=model, temperature=temperature)
        return {"status": "success", "generated_text": generated_text}

    except Exception as e:
        logger.error(f"继续生成失败: {str(e)}")
        logger.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}

def split_text_into_chunks(text: str, max_chunk_size: int = 4000) -> list[str]:
    """将长文本分割成较小的块，确保每个块的大小不超过max_chunk_size"""
    # 按段落分割
    paragraphs = text.split('\n\n')
    chunks = []
    current_chunk = []
    current_size = 0
    
    for paragraph in paragraphs:
        paragraph_size = len(paragraph)
        if current_size + paragraph_size > max_chunk_size and current_chunk:
            # 当前块已满，保存并开始新块
            chunks.append('\n\n'.join(current_chunk))
            current_chunk = [paragraph]
            current_size = paragraph_size
        else:
            # 添加到当前块
            current_chunk.append(paragraph)
            current_size += paragraph_size
    
    # 添加最后一个块
    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))
    
    return chunks

async def analyze_text_chunks(chunks: List[str], model: str, temperature: float) -> str:
    """
    分析文本块并生成综合分析结果
    """
    analysis_results = []
    
    for i, chunk in enumerate(chunks):
        logger.info(f"处理第 {i+1}/{len(chunks)} 个文本块")
        chunk_prompt = f"请分析以下文本并提供见解：\n\n{chunk}"
        
        try:
            result = await call_ai_with_retry(chunk_prompt, model=model, temperature=temperature)
            if isinstance(result, dict) and 'content' in result:
                analysis_results.append(result['content'])
            elif isinstance(result, str):
                analysis_results.append(result)
            else:
                logger.error(f"意外的响应格式：{result}")
                raise ValueError(f"AI 服务返回了意外的响应格式：{result}")
                
        except Exception as e:
            logger.error(f"处理文本块 {i+1} 失败：{str(e)}")
            logger.error(traceback.format_exc())
            # 不立即失败，记录错误并继续处理其他块
            analysis_results.append(f"[处理此部分时出错: {str(e)}]")
            continue
    
    if not analysis_results:
        raise HTTPException(
            status_code=503,
            detail="所有文本块处理均失败"
        )
    
    # 合并所有分析结果
    try:
        combined_analysis = '\n\n---\n\n'.join(
            result for result in analysis_results 
            if not result.startswith('[处理此部分时出错')
        )
        
        if not combined_analysis.strip():
            raise HTTPException(
                status_code=503,
                detail="生成的内容为空"
            )
            
        logger.info("成功合并所有分析结果")
        return combined_analysis
        
    except Exception as e:
        logger.error(f"合并分析结果时出错：{str(e)}")
        logger.error(f"分析结果：{analysis_results}")
        raise HTTPException(
            status_code=500,
            detail=f"合并分析结果失败：{str(e)}"
        )

@app.post("/analyze")
async def analyze_endpoint(
    resume: UploadFile = File(...),
    personal_statement: UploadFile = File(...),
    school_info: UploadFile = File(...),
    model: str = Form("deepseek-api"),
    temperature: float = Form(0.7)
):
    """分析材料端点"""
    try:
        logger.info(f"收到分析请求 - 文件: {resume.filename}, {personal_statement.filename}, {school_info.filename}")
        logger.info(f"模型: {model}, Temperature: {temperature}")
        
        # 读取文件内容
        resume_text = await process_file(resume)
        ps_text = await process_file(personal_statement)
        school_info_data = await process_file(school_info, is_school_info=True)
        
        # 分析材料
        analysis_result = await analyze_materials(
            resume_text,
            ps_text,
            school_info_data,
            model,
            temperature
        )
        
        return {
            "status": "success",
            "analysis": analysis_result
        }
        
    except Exception as e:
        logger.error(f"分析请求失败: {str(e)}")
        logger.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}

async def analyze_materials(resume_text: str, ps_text: str, school_info_data: dict, model: str, temperature: float) -> str:
    """分析材料并提取关键信息"""
    try:
        # 提取专业信息
        undergrad_major, target_major = extract_majors_from_school_info(school_info_data)
        
        # 构建分析提示词
        analysis_prompt = f"""请分析以下申请材料，提取关键信息和亮点，并标注信息的来源（如：简历调查表、个人陈述调查表、申请学校信息表等）为生成个人陈述做准备。
请重点关注一下几个方面和目标专业的匹配度：
1. 申请人的学术背景和专业知识
2. 相关的项目经历和实习经验
3. 研究经历和成果
4. 与目标专业的相关性
5. 个人特质和软实力

申请人背景：
- 本科专业：{undergrad_major}
- 目标专业：{target_major}

简历信息：
{resume_text}

个人陈述调查表信息：
{ps_text}

申请学校信息：
{json.dumps(school_info_data, ensure_ascii=False, indent=2)}

请提供详细的分析，包括：
1. 申请人的主要优势和特点
2. 可以重点突出的经历和成果
3. 需要特别说明或解释的内容
4. 对个人陈述写作的整体思路"""

        # 调用 AI 进行分析
        logger.info("开始分析材料")
        analysis_result = await call_ai_with_retry(analysis_prompt, model=model, temperature=temperature)
        logger.info("材料分析完成")
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"材料分析失败: {str(e)}")
        logger.error(traceback.format_exc())
        raise

async def call_ai_with_retry_stream(prompt: str, model: str = "deepseek-api", temperature: float = 1.5, max_retries: int = 3):
    """调用AI服务生成文本，支持流式输出"""
    logger.debug("开始调用 AI 服务（流式）")
    logger.info(f"模型：{model}，temperature：{temperature}")
    logger.debug(f"提示词长度：{len(prompt)} 字符")
    
    retry_count = 0
    last_error = None
    timeout = httpx.Timeout(120.0, read=300.0)  # 增加初始超时设置：连接2分钟，读取5分钟
    
    while retry_count < max_retries:
        try:
            model_lower = model.lower().strip()
            logger.debug(f"尝试第 {retry_count + 1} 次调用，模型：{model_lower}")
            logger.debug(f"当前超时设置：连接 {timeout.connect} 秒，读取 {timeout.read} 秒")
            
            if any(m in model_lower for m in ["deepseek", "deepseek-api", "deepseek-chat"]):
                api_key = os.getenv("DEEPSEEK_API_KEY")
                if not api_key:
                    raise HTTPException(
                        status_code=503,
                        detail="DeepSeek API 密钥未设置，请检查环境变量"
                    )
                
                headers = {
                    "Authorization": f"Bearer {api_key}",
                    "Content-Type": "application/json",
                    "Accept": "text/event-stream"
                }
                
                request_data = {
                    "model": "deepseek-chat",
                    "messages": [
                        {"role": "system", "content": "You are a helpful assistant specialized in writing personal statements."},
                        {"role": "user", "content": prompt}
                    ],
                    "temperature": temperature,
                    "stream": True,
                    "max_tokens": 4000
                }
                
                try:
                    async with httpx.AsyncClient(timeout=timeout) as client:
                        async with client.stream(
                            "POST",
                            "https://api.deepseek.com/v1/chat/completions",
                            headers=headers,
                            json=request_data
                        ) as response:
                            response.raise_for_status()
                            buffer = ""  # 用于累积不完整的JSON
                            
                            async for raw_line in response.aiter_lines():
                                if raw_line:
                                    try:
                                        line = raw_line.strip()
                                        if line.startswith("data: "):
                                            line = line[6:].strip()
                                            if line:  # 确保不是空行
                                                try:
                                                    data = json.loads(line)
                                                    if "choices" in data and len(data["choices"]) > 0:
                                                        delta = data["choices"][0].get("delta", {})
                                                        if "content" in delta:
                                                            yield delta["content"]
                                                except json.JSONDecodeError as e:
                                                    # 处理不完整的JSON
                                                    buffer += line
                                                    try:
                                                        data = json.loads(buffer)
                                                        buffer = ""  # 重置buffer
                                                        if "choices" in data and len(data["choices"]) > 0:
                                                            delta = data["choices"][0].get("delta", {})
                                                            if "content" in delta:
                                                                yield delta["content"]
                                                    except json.JSONDecodeError:
                                                        continue  # 继续累积buffer
                                    except Exception as e:
                                        logger.error(f"处理响应行时出错：{str(e)}")
                                        logger.error(f"问题行内容：{raw_line[:200]}")
                                        continue
                            
                            if buffer:  # 处理最后可能剩余的buffer
                                try:
                                    data = json.loads(buffer)
                                    if "choices" in data and len(data["choices"]) > 0:
                                        delta = data["choices"][0].get("delta", {})
                                        if "content" in delta:
                                            yield delta["content"]
                                except json.JSONDecodeError:
                                    pass
                            
                            return
                            
                except httpx.TimeoutException as e:
                    logger.error(f"请求超时：{str(e)}")
                    raise ValueError(f"API 请求超时：{str(e)}")
                except httpx.HTTPStatusError as e:
                    logger.error(f"HTTP 错误：{e.response.status_code} - {e.response.text}")
                    raise ValueError(f"API 请求失败：{e.response.text}")
                except Exception as e:
                    logger.error(f"请求过程中出错：{str(e)}")
                    raise ValueError(f"API 请求过程中出错：{str(e)}")
                    
            else:
                raise ValueError(f"不支持的模型：{model}")
                
        except Exception as e:
            last_error = e
            retry_count += 1
            if retry_count < max_retries:
                wait_time = min(2 ** retry_count, 30)  # 最长等待30秒
                logger.warning(f"调用失败 ({retry_count}/{max_retries}): {str(e)}")
                logger.warning(f"等待 {wait_time} 秒后重试...")
                await asyncio.sleep(wait_time)
                
                # 增加超时时间
                timeout = httpx.Timeout(
                    120.0 * (retry_count + 1),  # 每次重试增加2分钟
                    read=300.0 * (retry_count + 1)  # 每次重试增加5分钟
                )
            else:
                break
    
    error_msg = f"在 {max_retries} 次尝试后调用仍然失败"
    if last_error:
        error_msg += f"：{str(last_error)}"
    logger.error(error_msg)
    raise HTTPException(status_code=503, detail=error_msg)

@app.post("/generate_ps")
async def generate_ps_endpoint(
    request: Request,
    model: str = Form("deepseek-api"),
    temperature: float = Form(1.5)
):
    """生成个人陈述端点"""
    try:
        # 获取请求数据
        data = await request.json()
        analysis = data.get("analysis")
        prompt_template = data.get("prompt_template")
        model = data.get("model", model)
        temperature = data.get("temperature", temperature)
        
        if not analysis or not prompt_template:
            raise HTTPException(status_code=400, detail="Missing required parameters")
        
        # 构建生成提示词
        generation_prompt = f"""基于以下分析结果和要求，生成一份完整的个人陈述。

分析结果：
{analysis}

要求和结构：
{prompt_template}

请确保：
1. 使用流畅的学术英语
2. 确保内容真实，基于上传文件中提供的信息
3. 通过具体例子而不是空泛的描述来展示能力
4. 突出与目标专业的契合度"""

        # 记录提示词长度
        prompt_length = len(generation_prompt)
        logger.info(f"生成提示词总长度：{prompt_length} 字符")
        logger.debug(f"分析结果长度：{len(analysis)} 字符")
        logger.debug(f"模板长度：{len(prompt_template)} 字符")

        async def generate():
            try:
                async for chunk in call_ai_with_retry_stream(generation_prompt, model=model, temperature=temperature):
                    yield f"data: {json.dumps({'text': chunk})}\n\n"
            except Exception as e:
                logger.error(f"生成失败: {str(e)}")
                yield f"data: {json.dumps({'error': str(e)})}\n\n"

        return StreamingResponse(
            generate(),
            media_type="text/event-stream"
        )
        
    except Exception as e:
        logger.error(f"生成请求失败: {str(e)}")
        logger.error(traceback.format_exc())
        return {"status": "error", "message": str(e)}

async def process_file(file: UploadFile, is_school_info: bool = False) -> str:
    """处理上传的文件并返回内容"""
    file_path = None
    try:
        # 读取上传的文件内容
        content = await file.read()
        
        # 创建临时文件
        suffix = Path(file.filename).suffix.lower()
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            file_path = tmp.name
            # 写入临时文件
            tmp.write(content)
            
        # 根据文件类型处理内容
        if suffix == '.txt':
            # 尝试不同的编码方式读取文本文件
            encodings = ['utf-8', 'gbk', 'gb2312', 'iso-8859-1']
            text_content = None
            for encoding in encodings:
                try:
                    text_content = content.decode(encoding)
                    break
                except UnicodeDecodeError:
                    continue
            if text_content is None:
                raise ValueError(f"Unable to decode {file.filename} with any supported encoding")
            return text_content
            
        elif suffix == '.docx':
            # 处理 Word 文档
            doc = docx.Document(file_path)
            # 读取段落文本
            paragraphs_text = [paragraph.text for paragraph in doc.paragraphs if paragraph.text.strip()]
            # 读取表格内容
            tables_text = []
            for table in doc.tables:
                for row in table.rows:
                    row_text = [cell.text.strip() for cell in row.cells if cell.text.strip()]
                    if row_text:  # 只添加非空行
                        tables_text.append(" | ".join(row_text))
            # 合并段落和表格内容
            text_content = "\n\n".join(paragraphs_text)
            if tables_text:
                text_content += "\n\n表格内容：\n" + "\n".join(tables_text)
            return text_content
            
        elif suffix in ['.xls', '.xlsx', '.csv']:
            # 处理电子表格
            if suffix == '.csv':
                df = pd.read_csv(file_path)
            else:
                df = pd.read_excel(file_path)
            # 将 DataFrame 转换为字符串格式
            return df.to_string()
            
        else:
            raise ValueError(f"Unsupported file type: {suffix}")
            
    except Exception as e:
        logger.error(f"Error processing file {file.filename}: {str(e)}")
        logger.error(traceback.format_exc())
        raise
        
    finally:
        # 清理临时文件
        if file_path and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except Exception as e:
                logger.error(f"Error cleaning up temporary file: {str(e)}")

@app.post("/analyze_stream")
async def analyze_materials_stream(
    resume: UploadFile = File(...),
    personal_statement: UploadFile = File(...),
    school_info: UploadFile = File(...),
    model: str = Form("deepseek-api"),
    temperature: float = Form(0.7),
    prompt_template: str = Form(None)
):
    """流式分析材料，提取关键信息点"""
    # 在开始流式响应前，先读取所有文件内容
    try:
        logger.info("开始处理流式分析请求")
        logger.info(f"接收到的文件：resume={resume.filename}, ps={personal_statement.filename}, school={school_info.filename}")
        
        # 读取所有文件内容到内存
        logger.info("读取文件内容到内存")
        resume_content = await resume.read()
        ps_content = await personal_statement.read()
        school_info_content = await school_info.read()
        
        logger.info(f"文件大小：resume={len(resume_content)}字节, ps={len(ps_content)}字节, school={len(school_info_content)}字节")
        
        # 重置文件指针，以防后续还需要读取
        await resume.seek(0)
        await personal_statement.seek(0)
        await school_info.seek(0)
        
        async def generate():
            try:
                logger.info("开始生成响应")
                # 创建临时文件并写入内容
                resume_path = Path(tempfile.gettempdir()) / f"resume_{resume.filename}"
                ps_path = Path(tempfile.gettempdir()) / f"ps_{personal_statement.filename}"
                school_info_path = Path(tempfile.gettempdir()) / f"school_{school_info.filename}"
                
                try:
                    # 写入临时文件
                    logger.info("写入临时文件")
                    await write_bytes_to_file(resume_path, resume_content)
                    await write_bytes_to_file(ps_path, ps_content)
                    await write_bytes_to_file(school_info_path, school_info_content)
                    
                    # 处理文件内容
                    logger.info("开始处理文件内容")
                    resume_text = read_document(str(resume_path))
                    logger.info("简历处理完成")
                    ps_text = read_document(str(ps_path))
                    logger.info("个人陈述处理完成")
                    school_info_data = read_school_info(str(school_info_path))
                    logger.info("学校信息处理完成")
                    
                    # 从学校信息中提取专业信息
                    undergrad_major, target_major = extract_majors_from_school_info(school_info_data)
                    logger.info(f"提取到的专业信息：本科={undergrad_major}, 目标={target_major}")
                    
                    # 构建分析提示词
                    logger.info("构建分析提示词")
                    analysis_prompt = f"""请分析以下申请材料，提取关键信息和亮点，并标注信息的来源（如：简历调查表、个人陈述调查表、申请学校信息表等）为生成个人陈述做准备。
请重点关注一下几个方面和目标专业的匹配度：
1. 申请人的学术背景和专业知识
2. 相关的项目经历和实习经验
3. 研究经历和成果
4. 与目标专业的相关性
5. 个人特质和软实力

申请人背景：
- 本科专业：{undergrad_major}
- 目标专业：{target_major}

简历信息：
{resume_text}

个人陈述调查表信息：
{ps_text}

申请学校信息：
{json.dumps(school_info_data, ensure_ascii=False, indent=2)}

请提供详细的分析，包括：
1. 申请人的主要优势和特点
2. 可以重点突出的经历和成果
3. 需要特别说明或解释的内容
4. 对个人陈述写作的整体思路"""

                    logger.info("开始调用AI服务")
                    # 使用流式输出
                    async for chunk in call_ai_with_retry_stream(analysis_prompt, model=model, temperature=temperature):
                        logger.debug(f"生成内容片段：{chunk[:50]}...")
                        yield f"data: {json.dumps({'text': chunk})}\n\n"
                finally:
                    # 清理临时文件
                    logger.info("清理临时文件")
                    for path in [resume_path, ps_path, school_info_path]:
                        if path and path.exists():
                            try:
                                path.unlink()
                                logger.info(f"删除临时文件：{path}")
                            except Exception as e:
                                logger.error(f"Error cleaning up temporary file {path}: {str(e)}")
            except Exception as e:
                logger.error(f"流式生成失败: {str(e)}")
                logger.error(traceback.format_exc())
                yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
        logger.info("返回StreamingResponse")
        return StreamingResponse(
            generate(),
            media_type="text/event-stream"
        )
        
    except Exception as e:
        logger.error(f"文件读取失败: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(status_code=500, detail=str(e))

async def write_bytes_to_file(path: Path, content: bytes):
    """异步写入文件内容"""
    async with aiofiles.open(path, 'wb') as f:
        await f.write(content)

def extract_majors_from_school_info(school_info_data: dict) -> tuple[str, str]:
    """从学校信息中提取本科专业和目标专业
    
    Args:
        school_info_data: 包含学校信息的字典
    
    Returns:
        tuple[str, str]: (本科专业, 目标专业)
    """
    try:
        # 直接查找对应的列名和值
        undergrad_major = school_info_data.get("在读专业", "")
        if not undergrad_major:
            undergrad_major = school_info_data.get("本科专业", "")
        if not undergrad_major:
            undergrad_major = school_info_data.get("当前专业", "A专业")
            
        target_major = school_info_data.get("申请专业", "")
        if not target_major:
            target_major = school_info_data.get("目标专业", "")
        if not target_major:
            target_major = school_info_data.get("意向专业", "B专业")
        
        logger.info(f"提取到的专业信息 - 在读专业: {undergrad_major}, 申请专业: {target_major}")
        return (undergrad_major, target_major)
        
    except Exception as e:
        logger.error(f"提取专业信息时出错: {str(e)}")
        return ("A专业", "B专业")  # 使用默认值

async def process_materials(resume_text: str, ps_text: str, school_info_data: dict, model: str, temperature: float, custom_prompt: str = None) -> str:
    """统一的材料处理函数，会自动判断是否需要分块处理"""
    
    # 标准化模型名称
    model = model.lower().strip()
    
    # 从学校信息中提取专业信息
    undergrad_major, target_major = extract_majors_from_school_info(school_info_data)
    
    # 合并所有材料
    combined_text = f"""简历信息：
{resume_text}

个人陈述调查表信息：
{ps_text}

申请学校信息：
{json.dumps(school_info_data, ensure_ascii=False, indent=2)}"""

    # 判断是否需要分块处理
    if len(combined_text) > MAX_CHUNK_SIZE:
        logger.info(f"文本长度({len(combined_text)})超过限制，使用分块处理")
        # 分块处理
        chunks = split_text_into_chunks(combined_text)
        return await analyze_text_chunks(chunks, model, temperature)
    else:
        logger.info(f"文本长度({len(combined_text)})在限制内，使用整体处理")
        # 整体处理
        prompt = f"""我是本科学{undergrad_major}专业的学生，想要申请{target_major}专业，请帮我写一份personal statement。请基本按照以下结构组织内容，你也可以根据内容进行相应的删减或补充，帮我重新组织经历，语言和结构，帮我拓展内容，逻辑清晰，展现我对目标专业的热情，理解和思考。
1. 开篇：介绍申请动机，可以结合对于专业未来方向的发展和思考，展现对目标专业的理解和热情
2. 结合申请人实际的过往经历来谈和申请项目的匹配度。包括但不限于：学术背景、项目经历、实习/研究、个人特质。请注意，描述需要详略得当，最重要的应该是申请人的项目/实习/研究和目标专业的匹配度，而不是申请人的个人特质和所上的课程。
3. 重申申请动机，展望未来规划

要求：
1. 使用流畅的学术英语
2. 确保内容真实，基于上传文件中提供的信息
3. 通过具体例子而不是空泛的描述来展示能力
4. 突出与目标专业的契合度

{custom_prompt if custom_prompt else ""}

材料内容：
{combined_text}
"""
        # 使用流式输出
        full_response = ""
        async for chunk in call_ai_with_retry_stream(prompt, model=model, temperature=temperature):
            full_response += chunk
        return full_response